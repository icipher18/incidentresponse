Potential Problems

Problems that may arise when using vulnerability analysis tools include:

■■ False positives: When scanners use generalized tests or if the scanner does not have the ability to deeply scan the application, it might not be able to determine whether the application actually has vulnerability. It might result in information that says the application might have vulnerability. If it sees that the server is running a remote control application, the test software may indicate that you have a “High” vulnerability. However, if you have taken care to implement the remote control application to a high standard, the organization’s vulnerability is not as high.

■■ Weeding out false positives: Even if a scanner reports a service as vulnerable or missing a patch that leads to vulnerability, the system is not necessarily vulnerable. Accuracy is a function of the scanner’s quality, that is, how complete and concise the testing mechanisms are built (better tests equal better results), how up to date the testing scripts are (fresher scripts are more likely to spot a fuller range of known problems), and how well it performs OS fingerprinting (knowing which OS the host runs helps the scanner pinpoint issues for applications that run on that OS). Double-check the scanner’s work. Verify that a claimed vulnerability is an actual vulnerability. Good scanners will reference documents to help you learn more about the issue.

■■ Crash exposure: V/A software has some inherent dangers because much of the vulnerability testing software includes denial-of-service test scripts (as well as other scripts), which, if used carelessly, can crash hosts. Ensure that hosts being tested have proper backups and that you test during times that will have the lowest impact on business operations.

■■ Temporal information: Scans are temporal in nature, which means that the scan results you have today become stale as time moves on and new vulnerabilities are discovered. Therefore, scans must be performed periodically with scanners that are up to date with the latest vulnerability signatures.
Host Scanning

Organizations serious about security create hardened host configuration procedures and use policy to mandate host deployment and change. There are many ingredients to creating a secure host, but remember that what is secure today might not be secure tomorrow, as conditions are ever changing.
Host Security Considerations

There are several areas to consider when securing a host or when evaluating its security:

■■ Disable unneeded services: Services that are not critical to the role the host serves should be disabled or removed as appropriate for that platform. For the services the host does offer, make sure it is using server programs considered secure, make sure you fully understand them, and tighten the configuration files to the highest degree possible. Unneeded services are often installed and left at their defaults, but since they are not needed, administrators ignore or forget about them. This may draw unwanted data traffic to the host from other hosts attempting connections, and it will leave the host vulnerable to weaknesses in the services.

If a host does not need a particular host process for its operation, do not install it. If software is installed but not used or intended for use on the machine, it may not be remembered or documented that software is on the machine and therefore will likely not be patched. Port mapping programs use many techniques to discover services available on a host. These results should be compared with the policy that defines this host and its role. One must continually ask the critical questions, for the less a host offers as a service to the world while still maintaining its job, the better for its security (because there is less chance of subverting extraneous applications).

■■ Disable insecure services: Certain programs used on systems are known to be insecure, cannot be made secure, and are easily exploitable; therefore, only use secure alternatives. These applications were developed for private, secure LAN environments, but as connectivity proliferated worldwide, their use has been taken to insecure communication channels.

Their weakness falls into three categories:

■■ They usually send authentication information unencrypted. For example, FTP and Telnet send username and passwords in the clear.

■■ They usually send data unencrypted. For example, HTTP sends data from client to server and back again entirely in the clear. For many applications, this is acceptable; however, for some it is not. SMTP also sends mail data in the clear unless it is secured by the application (e.g., the use of Pretty Good Privacy [PGP] within Outlook).

■■ Common services are studied carefully for weaknesses by people motivated to attack the organization’s systems.

Therefore, to protect hosts, one must understand the implications of using these and other services that are commonly “hacked.” Eliminate them when necessary or substitute them for more secure versions. For example:

■■ To ensure privacy of login information as well as the contents of client to server transactions, use SSH (secure shell) to log in to hosts remotely instead of Telnet.

■■ Use SSH as a secure way to send insecure data communications between hosts by redirecting the insecure data into an SSH wrapper. The details for doing this are different from system to system.

■■ Use SCP (Secure Copy) instead of FTP (file transfer protocol).

■■ Ensure least privilege file system permissions: Least privilege is the concept that describes the minimum number of permissions required to perform a particular task. This applies to services/daemon processes as well as user permissions. Often systems installed out of the box are at minimum security levels. Make an effort to understand how secure newly installed configurations are, and take steps to lock down settings using vendor recommendations.

■■ Make sure file system permissions are as tight as possible: For UNIXbased systems, remove all unnecessary SUID (set used ID) and SGID (set group ID) programs that embed the ability for a program running in one user context to access another program. This ability becomes even more dangerous in the context of a program running with root user permissions as a part of its normal operation.

For Windows-based systems, use the Microsoft Management Center (MMC) “security configuration and analysis” and “security templates” snap-ins to analyze and secure multiple features of the operation system, including audit and policy settings and the registry.

■■ Establish and enforce a patching policy: Patches are pieces of software code meant to fix a vulnerability or problem that has been identified in a portion of an operating system or in an application that runs on a host. Keep the following in mind regarding patching:

Patches should be tested for functionality, stability, and security. The SSCP should also ensure that the patch does not change the security configuration of the organization’s host. Some patches might reinstall a default account or change configuration settings back to a default mode. The SSCP will need a way to test whether new patches will break a system or an application running on a system. When patching highly critical systems, it is advised to deploy the patch in a test environment that mimics the real environment. If you do not have this luxury, only deploy patches at noncritical times, have a back-out plan, and apply patches in steps (meaning one-by-one) to ensure that each one was successful and the system is still operating.

Use patch reporting systems that evaluate whether systems have patches installed completely and correctly and which patches are missing. Many vulnerability analysis tools have this function built into them, but be sure to understand how often the V/A tool vendor updates this list versus another vendor who specializes in patch analysis systems. Some vendors have better updating systems than others.

Optimally, tools should test to see if once a patch has been applied to remove a vulnerability, the vulnerability does not still exist. Patch application sometimes includes a manual remediation component, such as a registry change or removing a user, and if the IT person applied the patch but did not perform the manual remediation component, the vulnerability may still exist.

■■ Examine applications for weaknesses: Applications should prevent privilege escalation and buffer overflows and myriad other threatening problems. However, this is not always the case, and applications need to be evaluated for their ability not to compromise a host. Insecure services and daemons that run on hardened hosts may by nature weaken the host. Applications should come from trusted sources. Similarly, it is inadvisable to download executables from websites you know nothing about. Executables should be hashed and verified with the publisher.

Signed executables also provide a level of assurance regarding the integrity of the file. Some programs can help evaluate a host’s applications for problems. In particular, these focus on web-based systems and database systems:

■■ Nikto evaluates web CGI systems for common and uncommon vulnerabilities in implementation

■■Web Inspect is an automated web server scanning tool

■■ AppDetective from evaluates applications, especially various types of databases, for vulnerability

You should also:

■■ Ensure that antivirus and antimalware is installed and is up to date with

the latest scan engine and pattern file offered by the vendor.

■■ Use products that encourage easy management and updates of signatures; otherwise, the systems may fail to be updated, rendering them ineffective to new exploits.

■■ Use products that centralize reporting of problems to spot problem areas and trends.

■■ Use system logging. Logging methods are advisable to ensure that system events are noted and securely stored, in the event they are needed later.

■■ Subscribe to vendor information. Vendors often publish information regularly, not only to keep their name in front of you but also to inform you of security updates and best practices for configuring their systems. Some tools also specialize in determining when a system’s software platform is out of compliance with the latest patches.

■■ Firewall and Router Testing: Firewalls are designed to be points of data restriction (choke points) between security domains. They operate on a set of rules driven by a security policy to determine what types of data are allowed from one side to the other (point A to point B) and back again (point B to point A).

Similarly, routers can also serve some of these functions when configured with Access Control Lists (ACLs). Organizations deploy these devices to not only connect network segments together, but also to restrict access to only those data flows that are required. This can help protect organizational data assets. Routers with ACLs, if used, are usually placed in front of the firewalls to reduce the noise and volume of traffic hitting the firewall. This allows the firewall to be more thorough in its analysis and handling of traffic. This strategy is also known as layering or defense-in- depth.

Changes to devices should be governed by change control processes that specify what types of changes can occur and when they can occur. This prevents haphazard and dangerous changes to devices that are designed to protect internal systems from other potentially hostile networks, such as the Internet or an extranet to which the organization’s internal network connects. Change control processes should include security testing to ensure the changes were implemented correctly and as expected.

Configuration of these devices should be reflected in security procedures, and the rules of the Access Control Lists should be engendered by organizational policy. The point of testing is to ensure machine configurations match approved policy.

A sample baseline for the Internet perimeter systems is given below. Edge routers will have the following qualities:

■■ For management: Telnet disabled; SSH enabled

■■ An authentication system that verifies that the person logging onto the router (for managing it) is who he/she says; accomplished with one-time password system

■■ An authorization system that verifies that the logged-on administrator has the privileges to perform the management routines he/she is attempting to invoke

■■ An accounting system that tracks the commands that were invoked; this forms the audit trail

■■ Basic intrusion detection signature recognition functionality

■■ Syslog event reporting to an internal host

■■ Blocking of RFC1918 (non-routable addresses) and packets sourced from 0.0.0.0 inbound and outbound

■■ Blocking of inbound MS networking, MS SQL communication, TFTP, Oracle SQL*Net, DHCP, all types of ICMP packets except for path MTU and echo replies; it should be noted that some of these ports may be necessary for business operations and they must be examined on a case-by-case basis before blocking

Firewalls will have the following qualities:

■■ For management: Telnet disabled; SSH or SSL enabled

■■ An authentication system that verifies that the person logging on to the firewall (for managing it) is who he/she says; accomplished with onetime password system

■■ An authorization system that verifies that the logged-on administrator has the privileges to perform the management routines he/she is attempting to invoke

■■ An accounting system that tracks the commands that were invoked (this forms the audit trail)

■■ Event report logging to an internal host

■■ Network address translation functionality, if required, is working properly

■■ Enabling inbound transmissions from anywhere to the organizational web server, FTP server, SMTP mail server, and e-commerce server (for example)

■■ Enabling inbound transmissions back to internal users who originally established the connections

■■ Enabling outbound HTTP, HTTPS, FTP, DNS from anyone on the inside (if approved in the policy)

■■ Enabling outbound SMTP from the mail server to any other mail server

■■ Blocking all other outbound access

With this sample baseline in mind, port scanners and vulnerability scanners can be leveraged to test the choke point’s ability to filter as specified. If internal (trusted) systems are reachable from the external (untrusted) side in ways not specified by policy, a mismatch has occurred and should be assessed. Likewise, internal to external testing should conclude that only the allowed outbound traffic could occur. The test should compare devices logs with the tests dispatched from the test host.

Advanced firewall testing will test a device’s ability to perform the following (this is a partial list and is a function of the firewall’s capabilities):

■■ Limit TCP port scanning reconnaissance techniques, including SYN, FIN, XMAS, NULL via the firewall.

■■ Limit ICMP and UDP port scanning reconnaissance techniques.

■■ Limit overlapping packet fragments.

■■ Limit half-open connections to trusted side devices. Attacks like these are called SYN attacks, when the attacker begins the process of opening many connections but never completes any of them, eventually exhausting the target host’s memory resources.

Advanced firewall testing can leverage a vulnerability or port scanner’s ability to dispatch denial-of-service and reconnaissance tests. A scanner can be configured to direct, for example, massive amounts of SYN packets at an internal host. If the firewall is operating properly and effectively, it will limit the number of these half-open attempts by intercepting them so that the internal host is not adversely affected. These tests must be used with care, since there is always a chance that the firewall will not do what is expected and the internal hosts might be affected.

■■ Security Monitoring Testing: IDS systems are technical security controls designed to monitor for and alert on the presence of suspicious or disallowed system activity within host processes and across networks. Device logging is used for recording many types of events that occur within hosts and network devices. Logs, whether generated by IDS or hosts, are used as audit trails and permanent records of what happened and when. Organizations have a responsibility to ensure that their monitoring systems are functioning correctly and alerting on the broad range of communications commonly in use. Documenting this testing can also be used to show due diligence. Likewise, you can use testing to confirm that IDS detects traffic patterns as claimed by the vendor.

With regard to IDS testing, methods should include the ability to provide a stimulus (i.e., send data that simulate an exploitation of a particular vulnerability) and observe the appropriate response by the IDS. Testing can also uncover an IDS’s inability to detect purposeful evasion techniques that might be used by attackers. Under controlled conditions, stimulus can be crafted and sent from vulnerability scanners. Response can be observed in log files generated by the IDS or any other monitoring system used in conjunction with the IDS. If the appropriate response is not generated, investigation of the causes can be undertaken.

With regard to host logging tests, methods should also include the ability to provide a stimulus (i.e., send data that simulates a “log-able” event) and observe the appropriate response by the monitoring system. Under controlled conditions, stimulus can be crafted in many ways depending on your test. For example, if a host is configured to log an alert every time an administrator or equivalent logs on, you can simply log on as the “root” user to the organization’s UNIX system. In this example, response can be observed in the system’s log files. If the appropriate log entry is not generated, investigation of the causes can be undertaken.

The overall goal is to make sure the monitoring is configured to the organization’s specifications and that it has all of the features needed.
Traffic Types

Consider testing for the following traffic types and conditions in an IDS environment, as vulnerability exploits can be contained within any of them. If the monitoring systems in use do not cover all of them, organization’s systems are open to exploitation:

■■ Data patterns that are contained within single packets: This is considered a minimum functionality since the IDS need only search through a single packet for an exploit.

■■ Data patterns contained within multiple packets: This is considered a desirable function since there is often more than one packet in a data stream between two hosts. This function, stateful pattern matching, requires the IDS to “remember” packets it saw in the past to reassemble them, as well as perform analysis to determine if exploits are contained within the aggregate payload.

■■ Obfuscated data: This refers to data that is converted from ASCII to Hexadecimal or Unicode characters and then sent in one or more packets. The IDS must be able to convert the code among all of these formats. If a signature that describes an exploit is written in ASCII but the exploit arrives at the organization’s system in Unicode, the IDS must convert it back to ASCII to recognize it as an exploit.

■■ Fragmented data: IP data can be fragmented across many small packets, which are then reassembled by the receiving host. Fragmentation occasionally happens in normal communications. In contrast, overlapping fragments is a situation where portions of IP datagrams overwrite and supersede one another as they are reassembled on the receiving system (a teardrop attack). This can wreak havoc on a computer, which can become confused and overloaded during the reassembly process. IDS must understand how to reassemble fragmented data and overlapping fragmented data so it can analyze the resulting data. These techniques are employed by attackers to subvert systems and to evade detection.

■■ Protocol embedded attacks: IDS should be able to decode (i.e., break apart, understand, and process) commonly used applications (DNS, SSL, HTTP, FTP, SQL, etc.) just like a host would, to determine whether an attacker has manipulated code that might crash the application or host on which it runs. Therefore, testing should employ exploits embedded within application data.

■■ Flooding detection: An IDS should be able to detect conditions indicative of a denial-of-service flood, when too many packets originate from one or more sources to one or more destinations. Thresholds are determined within the configuration. For example, an IDS should be able to detect if more than 10 half-open connections are opened within 2 seconds to any one host on the organization’s network.

■■ Intrusion Prevention Systems Security (IPS) Monitoring: IPSs are technical security controls designed to monitor and alert for the presence of suspicious or disallowed system activity within host processes and across networks, and then take action on suspicious activities. Likewise, you can use testing to confirm that IPS detects traffic patterns and reacts as claimed by the vendor. When auditing an IPS, its position in the architecture is slightly different from that of an IDS; an IPS needs to be positioned inline of the traffic flow so the appropriate action can be taken. Some of the other key differences are: the IPS acts on issues and handles the problems, while an IDS only reports on the traffic and requires some other party to react to the situation. The negative consequence of the IPS is that it is possible to reject good traffic and there will only be the logs of the IPS to show why the good traffic is getting rejected. Many times the networking staff may not have access to those logs and may find network troubleshooting more difficult.

■■ Security Gateway Testing: Some organizations use security gateways or web proxies to intercept certain communications and examine them for validity. Gateways perform their analysis on these communications based on a set of rules supplied by the organization — rules driven by policy — and pass them along if they are deemed appropriate and exploitation-free, or block them if they are not.